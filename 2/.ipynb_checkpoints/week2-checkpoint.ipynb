{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Sklearn调用TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
      " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n",
      "[[0.         0.         0.         0.34884223 0.44246214 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.69768446 0.44246214 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.4533864  0.35745504 0.         0.35745504\n",
      "  0.35745504 0.         0.         0.4533864  0.         0.\n",
      "  0.         0.         0.4533864  0.         0.         0.\n",
      "  0.        ]\n",
      " [0.5        0.5        0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.28113163\n",
      "  0.28113163 0.         0.35657982 0.         0.35657982 0.35657982\n",
      "  0.         0.35657982 0.         0.28113163 0.         0.35657982\n",
      "  0.35657982]]\n",
      "0.027725887222397813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['I come to China to travel',\n",
    "          'This is a car polupar in China',\n",
    "          'I love tea and Apple',\n",
    "          'The work is to write some papers in science']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).toarray())\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "print(tfidf.toarray())\n",
    "\n",
    "\n",
    "# 使用nltk\n",
    "from nltk.text import TextCollection\n",
    "\n",
    "corpus = ['I come to China to travel',\n",
    "          'This is a car polupar in China',\n",
    "          'I love tea and Apple',\n",
    "          'The work is to write some papers in science']\n",
    "\n",
    "corpus = TextCollection(corpus)\n",
    "# 直接就能算出TF-IDF\n",
    "print(corpus.tf_idf('China', 'I come to China to travel'))\n",
    "# 0.027725887222397813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:\n",
      " [[0.         0.52640543 0.         0.         0.         0.52640543\n",
      "  0.         0.         0.66767854 0.         0.         0.        ]\n",
      " [0.         0.         0.52547275 0.         0.         0.41428875\n",
      "  0.52547275 0.         0.         0.         0.         0.52547275]\n",
      " [0.4472136  0.         0.         0.         0.4472136  0.\n",
      "  0.         0.4472136  0.         0.4472136  0.4472136  0.        ]\n",
      " [0.         0.6191303  0.         0.78528828 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "word:\n",
      " ['中国', '北京', '大厦', '天安门', '小明', '来到', '杭研', '毕业', '清华大学', '硕士', '科学院', '网易']\n",
      "来到 5\n",
      "北京 1\n",
      "清华大学 8\n",
      "网易 11\n",
      "杭研 6\n",
      "大厦 2\n",
      "小明 4\n",
      "硕士 9\n",
      "毕业 7\n",
      "中国 0\n",
      "科学院 10\n",
      "天安门 3\n",
      "---------------------- 0 --------------------\n",
      "中国 0.0\n",
      "北京 0.5264054336099155\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.5264054336099155\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.6676785446095399\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.0\n",
      "---------------------- 1 --------------------\n",
      "中国 0.0\n",
      "北京 0.0\n",
      "大厦 0.5254727492640658\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.41428875116588965\n",
      "杭研 0.5254727492640658\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.5254727492640658\n",
      "---------------------- 2 --------------------\n",
      "中国 0.4472135954999579\n",
      "北京 0.0\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.4472135954999579\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.4472135954999579\n",
      "清华大学 0.0\n",
      "硕士 0.4472135954999579\n",
      "科学院 0.4472135954999579\n",
      "网易 0.0\n",
      "---------------------- 3 --------------------\n",
      "中国 0.0\n",
      "北京 0.6191302964899972\n",
      "大厦 0.0\n",
      "天安门 0.7852882757103967\n",
      "小明 0.0\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "tfidf = TfidfVectorizer()\n",
    " \n",
    "corpus=[\"我 来到 北京 清华大学\",          # 第一类文本切词后的结果，词之间以空格隔开\n",
    "        \"他 来到 了 网易 杭研 大厦\",      # 第二类文本的切词结果\n",
    "        \"小明 硕士 毕业 与 中国 科学院\",  # 第三类文本的切词结果\n",
    "        \"我 爱 北京 天安门\"]              # 第四类文本的切词结果\n",
    " \n",
    "result = tfidf.fit_transform(corpus).toarray()\n",
    "print('result:\\n', result)\n",
    "\n",
    "# 统计关键词\n",
    "word = tfidf.get_feature_names()\n",
    "print('word:\\n', word)\n",
    "\n",
    "# 统计关键词出现次数，几句话对比几次\n",
    "# k在vocabulary中的位置\n",
    "for k,v in tfidf.vocabulary_.items():\n",
    "    print(k, v)\n",
    "    \n",
    "# 对比第i类文本的词语tf-idf权重\n",
    "for i in range(len(result)):\n",
    "    print('----------------------',i,'--------------------')\n",
    "    for j in range(len(word)):\n",
    "        print(word[j], result[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词\n",
    "\n",
    "- nltk\n",
    "\n",
    "- jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "Arthur didn't feel very good.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "# ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    "\n",
    "# 词性标注\n",
    "targed = nltk.pos_tag(tokens)\n",
    "print(targed[0:6])\n",
    "\n",
    "# ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    "# [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.189 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut('我爱北京天安门')\n",
    "for word, flag in words:\n",
    "    print(word, flag)\n",
    "    \n",
    "# 我 r\n",
    "# 爱 v\n",
    "# 北京 ns\n",
    "# 天安门 ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他/ 来到/ 了/ 网易/ 杭研/ 大厦\n",
      "小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut('我来到北京清华大学', cut_all=True)\n",
    "print('Full Mode: ' + '/ '.join(seg_list))      # 全模式\n",
    "# Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
    "\n",
    "seg_list = jieba.cut('我来到北京清华大学', cut_all=False)\n",
    "print('Default Mode: ' + '/ '.join(seg_list))   # 精确模式\n",
    "# Default Mode: 我/ 来到/ 北京/ 清华大学\n",
    "\n",
    "seg_list = jieba.cut('他来到了网易杭研大厦')    # 默认是精确模式\n",
    "print('/ '.join(seg_list))\n",
    "# 他/ 来到/ 了/ 网易/ 杭研/ 大厦\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search('小明硕士毕业于中国科学院计算所，后在日本京都大学深造')\n",
    "print('/ '.join(seg_list))\n",
    "# 小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
